{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys, glob, string, shutil, re\n",
    "import yaml, json\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "\n",
    "class UnsortableList(list):\n",
    "    def sort(self, *args, **kwargs):\n",
    "        pass\n",
    "    \n",
    "class UnsortableOrderedDict(OrderedDict):\n",
    "    def items(self, *args, **kwargs):\n",
    "        return UnsortableList(OrderedDict.items(self, *args, **kwargs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OrderedDictYAMLLoader(yaml.Loader):\n",
    "    \"\"\"\n",
    "    A YAML loader that loads mappings into ordered dictionaries.\n",
    "    \"\"\"\n",
    " \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        yaml.Loader.__init__(self, *args, **kwargs)\n",
    " \n",
    "        self.add_constructor(u'tag:yaml.org,2002:map', type(self).construct_yaml_map)\n",
    "        self.add_constructor(u'tag:yaml.org,2002:omap', type(self).construct_yaml_map)\n",
    " \n",
    "    def construct_yaml_map(self, node):\n",
    "        data = OrderedDict()\n",
    "        yield data\n",
    "        value = self.construct_mapping(node)\n",
    "        data.update(value)\n",
    " \n",
    "    def construct_mapping(self, node, deep=False):\n",
    "        if isinstance(node, yaml.MappingNode):\n",
    "            self.flatten_mapping(node)\n",
    "        else:\n",
    "            raise yaml.constructor.ConstructorError(None, None,\n",
    "                'expected a mapping node, but found %s' % node.id, node.start_mark)\n",
    " \n",
    "        mapping = OrderedDict()\n",
    "        for key_node, value_node in node.value:\n",
    "            key = self.construct_object(key_node, deep=deep)\n",
    "            try:\n",
    "                hash(key)\n",
    "            except TypeError, exc:\n",
    "                raise yaml.constructor.ConstructorError('while constructing a mapping',\n",
    "                    node.start_mark, 'found unacceptable key (%s)' % exc, key_node.start_mark)\n",
    "            value = self.construct_object(value_node, deep=deep)\n",
    "            mapping[key] = value\n",
    "        return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_prov(content):\n",
    "\n",
    "    prov = re.search('_output_var_names\\s*=\\s*\\[(.*?)\\]',content, re.DOTALL)\n",
    "    if prov:\n",
    "        prov = re.split('\\n.*?',prov.groups()[0])\n",
    "        prov = [re.sub('\\s*##.*','',i) for i in prov]\n",
    "        prov = [re.sub('\\s*\\'','',i) for i in prov]\n",
    "        prov = [re.sub(',\\s*','',i) for i in prov]\n",
    "        prov = [re.sub('#\\s*.*','',i) for i in prov]\n",
    "        prov = [re.sub(' ','',i) for i in prov]\n",
    "        prov = [re.sub('\\\\r','',i) for i in prov]\n",
    "        prov = [i for i in prov if len(i)>0]\n",
    "        \n",
    "        return prov\n",
    "\n",
    "def find_uses(content):\n",
    "\n",
    "    uses = re.search('_input_var_names\\s*=\\s*\\[(.*?)\\]',content, re.DOTALL)\n",
    "    if uses:\n",
    "        uses = re.split('\\n.*?',uses.groups()[0])\n",
    "        uses = [re.sub('\\s*##.*','',i) for i in uses]\n",
    "        uses = [re.sub('\\s*\\'','',i) for i in uses]\n",
    "        uses = [re.sub(',\\s*','',i) for i in uses]\n",
    "        uses = [re.sub('#\\s*.*','',i) for i in uses]\n",
    "        uses = [re.sub(' ','',i) for i in uses]\n",
    "        uses = [re.sub('\\\\r','',i) for i in uses]\n",
    "        uses = [i for i in uses if len(i)>0]\n",
    "        \n",
    "        return uses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def match_keys(toKey,fromKey):\n",
    "\n",
    "    allU = UnsortableOrderedDict()\n",
    "\n",
    "    for k in toKey.keys():\n",
    "\n",
    "        allk = {}\n",
    "        vals = toKey[k]\n",
    "        keys_ = [[name for name in fromKey.keys() for i in fromKey[name] if i == j] for j in vals]\n",
    "        \n",
    "        \n",
    "        keys = [list({j.split('_')[0] for j in i}) if len(i)>0 else [''] for i in keys_]\n",
    "        \n",
    "        keys_lin = list({keys[i][j] for i in range(len(keys)) for j in range(len(keys[i]))})\n",
    "        keys_pairs = [[keys[i][j],vals[i]] for i in range(len(keys)) for j in range(len(keys[i]))]\n",
    "\n",
    "        allk = {keys_lin[i]:[] for i in range(len(keys_lin))}\n",
    "\n",
    "        for i in range(len(keys_pairs)):\n",
    "            allk[keys_pairs[i][0]].append(keys_pairs[i][1])\n",
    "\n",
    "        allU[k] = allk\n",
    "        \n",
    "    return allU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_provides_dot_json(comp_dir, prov):\n",
    "    # where prov is a list of dictionaries of the connections it provides\n",
    "    \n",
    "    Pls = []\n",
    "    \n",
    "    for k in prov.keys():\n",
    "    \n",
    "        p = UnsortableOrderedDict()\n",
    "        p['id'] = k\n",
    "        p['required'] = False\n",
    "        p['exchange_items'] = prov[k]\n",
    "        \n",
    "        Pls.append(p)\n",
    "    \n",
    "    \n",
    "    \n",
    "    o = open(comp_dir + '/provides.json', 'w')\n",
    "    json.dump(Pls, o, indent = 2, sort_keys = False)\n",
    "    o.close()\n",
    "    \n",
    "\n",
    "def create_uses_dot_json(comp_dir, uses):\n",
    "    # where uses is a list of dictionaries of the connections it needs\n",
    "    \n",
    "    Pls = []\n",
    "    \n",
    "    for k in uses.keys():\n",
    "    \n",
    "        p = UnsortableOrderedDict()\n",
    "        p['id'] = k\n",
    "        p['required'] = False\n",
    "        p['exchange_items'] = uses[k]\n",
    "        \n",
    "        Pls.append(p)\n",
    "    \n",
    "    o = open(comp_dir + '/uses.json', 'w')\n",
    "    json.dump(Pls, o, indent = 2, sort_keys = False)\n",
    "    o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating uses/provides for channels_base\n",
      "Updating uses/provides for channels_diffusive_wave\n",
      "Updating uses/provides for channels_dynamic_wave\n",
      "Updating uses/provides for channels_kinematic_wave\n",
      "Updating uses/provides for d8_base\n",
      "Updating uses/provides for d8_global\n",
      "Updating uses/provides for d8_local\n",
      "Updating uses/provides for diversions_base\n",
      "Updating uses/provides for diversions_fraction_method\n",
      "Updating uses/provides for diversions_fraction_method_LAST\n",
      "Updating uses/provides for erode_base\n",
      "Updating uses/provides for erode_d8_global\n",
      "Updating uses/provides for erode_d8_local\n",
      "Updating uses/provides for evap_base\n",
      "Updating uses/provides for evap_energy_balance\n",
      "Updating uses/provides for evap_priestley_taylor\n",
      "Updating uses/provides for evap_read_file\n",
      "Updating uses/provides for gc2d\n",
      "Updating uses/provides for HIS_base\n",
      "Updating uses/provides for HIS_test\n",
      "Updating uses/provides for ice_base\n",
      "Updating uses/provides for infil_base\n",
      "Updating uses/provides for infil_beven\n",
      "Updating uses/provides for infil_green_ampt\n",
      "Updating uses/provides for infil_richards_1D\n",
      "Updating uses/provides for infil_richards_1D_METHOD1\n",
      "Updating uses/provides for infil_smith_parlange\n",
      "Updating uses/provides for met_base\n",
      "Updating uses/provides for OLD_solar_funcs_gui\n",
      "Updating uses/provides for satzone_base\n",
      "Updating uses/provides for satzone_darcy_layers\n",
      "Updating uses/provides for smooth_DEM\n",
      "Updating uses/provides for snow_base\n",
      "Updating uses/provides for snow_degree_day\n",
      "Updating uses/provides for snow_energy_balance\n",
      "Updating uses/provides for soil_base\n",
      "Updating uses/provides for soil_funcs\n",
      "Updating uses/provides for solar_funcs\n",
      "Updating uses/provides for topoflow_driver\n"
     ]
    }
   ],
   "source": [
    "yaml_root = 'yaml/'\n",
    "components_root = 'components/'\n",
    "source_dir = 'topoflow/'\n",
    "\n",
    "source_dirs = glob.iglob(source_dir + '*.py') # py files\n",
    "\n",
    "Uses = UnsortableOrderedDict()\n",
    "Provides = UnsortableOrderedDict()\n",
    "\n",
    "for comp in source_dirs:\n",
    "    \n",
    "    comp_name = string.split(comp,'/')[-1][:-3]\n",
    "    \n",
    "    print 'Updating uses/provides for ' + comp_name\n",
    "\n",
    "    # make new dirs\n",
    "#     comp_dir = components_root + string.lower(comp_name)\n",
    "#     if not os.path.exists(comp_dir):\n",
    "#         os.makedirs(comp_dir)\n",
    "#         os.makedirs(comp_dir + '/db')\n",
    "    \n",
    "    with open(comp, 'r') as content_file:\n",
    "        content = content_file.read()\n",
    "    \n",
    "    # provides.json\n",
    "    provides = find_prov(content)\n",
    "    if provides:\n",
    "        Provides[comp_name] = provides\n",
    "    \n",
    "    # uses.json\n",
    "    uses = find_uses(content)\n",
    "    if uses:\n",
    "        Uses[comp_name] = uses\n",
    "    \n",
    "keyed_provides = match_keys(Provides,Uses)\n",
    "keyed_uses = match_keys(Uses,Provides)\n",
    "\n",
    "# o = open(components_root + '/_uses_extra.json', 'w')\n",
    "# json.dump(uses_extra, o, indent = 2, sort_keys = False)\n",
    "# o.close()\n",
    "# o = open(components_root + '/_provides_extra.json', 'w')\n",
    "# json.dump(provides_extra, o, indent = 2, sort_keys = False)\n",
    "# o.close()\n",
    "\n",
    "source_dirs = glob.iglob(source_dir + '*.py') # py files\n",
    "\n",
    "for comp in source_dirs:\n",
    "    \n",
    "    comp_name = string.split(comp,'/')[-1][:-3]\n",
    "    comp_dir = components_root + string.lower(comp_name)\n",
    "    \n",
    "    if any(i == comp_name for i in keyed_provides.keys()) or any(i == comp_name for i in keyed_uses.keys()):\n",
    "        \n",
    "        comp_dir = components_root + string.lower(comp_name)\n",
    "        if not os.path.exists(comp_dir):\n",
    "            os.makedirs(comp_dir)\n",
    "    \n",
    "        if any(i == comp_name for i in keyed_provides.keys()):\n",
    "            create_provides_dot_json(comp_dir, keyed_provides[comp_name])\n",
    "        if any(i == comp_name for i in keyed_uses.keys()):\n",
    "            create_uses_dot_json(comp_dir, keyed_uses[comp_name])\n",
    "\n",
    "\n",
    "# inBoth = set(Uses) & set(Provides)\n",
    "# inOneButNotOther = set(Uses) | set(Provides)\n",
    "# inUsesNotProvides = {i for i in Uses if set(i) | set(Provides)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
